{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Newton-Raphson Method"
      ],
      "metadata": {
        "id": "HzBE6Bt4CFHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Prelude**\n",
        "\n",
        "If you enter $\\sqrt{2}$ into a calculator, you'll probably see an output similar to $1.4142135623730950$. How does your calculator find this value?\n",
        "\n",
        "We're going to look at two methods today for solving for the zeros of a function. That is to say, if we have a (nice) function $f(x)$, we're going to develop methods that we can use to approximate input values $x=c$ where $f(c)=0$.\n",
        "\n",
        "The first method we'll look at, the Bisection Method, requires only that $f(x)$ is a continuous function. We'll repeatedly use the Intermediate Value Theorem to find closer and closer approximations to the zeros of a function, by analyzing the behavior of the function on smaller and smaller intervals.\n",
        "\n",
        "The second method we'll look at, the Newton-Raphson Method, requires that $f(x)$ is a differentiable function. This is a stronger requirement and we'll see that, in-turn, this method often allows for \"faster\" approximation of zeros. Often times, this method is how your calculator will find the square root of a number."
      ],
      "metadata": {
        "id": "iLMXFkT7CIsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Learning Goals**\n",
        "\n",
        "In this notebook, we're going to do the following:\n",
        "\n",
        "1. We will learn two methods for approximating the zeros of nice functions (the Bisection Method and the Newton-Raphson Method) using the concepts that we've learned from calculus so far.\n",
        "2. We will analyze the performance of both methods, using the tools from Python that we've learned so far."
      ],
      "metadata": {
        "id": "DvNavnpjCL8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. The Bisection Method and the Intermediate Value Theorem"
      ],
      "metadata": {
        "id": "nWYlumNve5v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with an example, say $f(x)=x^3+x-1$. Because this is an odd degree polynomial, we have\n",
        "\n",
        "$$\\lim_{x\\rightarrow -\\infty} f(x)=-\\infty \\quad \\mbox{and} \\quad \\lim_{x\\rightarrow \\infty} f(x) =\\infty.$$\n",
        "\n",
        "Since we know that polynomials are continuous, the change in sign tells us that our function crosses the $x$-axis at some point. We can get an initial guess for where this happens by plotting the graph on a large enough interval."
      ],
      "metadata": {
        "id": "Zxq0ky41ijw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "XoNyS-pRjr9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return x**3+x-1\n",
        "\n",
        "x_inputs=[]\n",
        "y_outputs=[]\n",
        "\n",
        "step=0.01\n",
        "for i in range(400):\n",
        "  x_inputs.append(-2+i*step)\n",
        "  y_outputs.append(f(x_inputs[i]))\n",
        "\n",
        "plt.grid(True) # This function is new, it adds a grid to our plot\n",
        "plt.axhline(0, color='black', linewidth=0.8) # This function is also new, it\n",
        "# adds a horizontal line at 0 to our plot\n",
        "plt.plot(x_inputs,y_outputs)"
      ],
      "metadata": {
        "id": "rg_ie0WFjvf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like there is a zero between $x=0.5$ and $x=1.0$. We can confirm this by checking these values, and appealing to the Intermediate Value Theorem."
      ],
      "metadata": {
        "id": "ocZLMIYAlrB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f(0.5)"
      ],
      "metadata": {
        "id": "xmUMYLcJmHHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(1.0)"
      ],
      "metadata": {
        "id": "ToXgri-ymJu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because the function is continuous and changes sign on the interval $[0.5,1]$, the Intermediate Value Theorem guarantees that there is a zero in this interval. Let's call this value $x=c$. Our goal is to get a good approximation for $c$.\n",
        "\n",
        "Since we don't know where the zero is in this interval, we may as well guess that it is in the middle. Therefore, an approximation to the zero is\n",
        "\n",
        "$$x_1=0.5+(1-0.5)/2=0.75.$$\n",
        "\n",
        "What is the *error* in our approximation? Well, the true value of $c$ could be very close to either $0.5$ or $1.0$. So, if we call $\\epsilon_1$ our error, then we only know:\n",
        "\n",
        "$$\\epsilon_1 = |x_1-c|\\leq |x_1-0.5|=|x_1-1.0|=0.25.$$\n",
        "\n",
        "To get a better approximation, with a smaller possible error, we can check the value of $f$ at $0.75$, and repeat this process for one of the resulting interval halves. At this point, let's make it more automated."
      ],
      "metadata": {
        "id": "yjVJbwNumSWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following will be our set-up."
      ],
      "metadata": {
        "id": "iJwuqCcZ9mAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll make a list that contains the endpoints of our interval\n",
        "I_1=[0.5, 1.0]\n",
        "\n",
        "# We'll make a function that acts like f\n",
        "def f(x):\n",
        "  return x**3+x-1\n",
        "\n",
        "# For convenience, let's also make this function:\n",
        "def sign(x):\n",
        "  if x == 0:\n",
        "    return 0\n",
        "  if x != 0:\n",
        "    return abs(x)/x # abs(x) is just another name for |x|\n",
        "\n",
        "def half_length(a,b):\n",
        "  return (b-a)/2\n",
        "\n",
        "# We'll make a function that takes in two numbers a,b\n",
        "# with a<b and returns the midpoint\n",
        "def midpoint(a,b):\n",
        "  return a+half_length(a,b)\n",
        "\n",
        "# We'll make a function that produces the next interval from our\n",
        "# previous one\n",
        "def bisect(I):\n",
        "  a=I[0]\n",
        "  b=I[1]\n",
        "\n",
        "  c=midpoint(a,b)\n",
        "\n",
        "  if sign(f(a)) == sign(f(c)):\n",
        "    I=[c,b]\n",
        "    return I\n",
        "\n",
        "  if sign(f(b)) == sign(f(c)):\n",
        "    I=[a,c]\n",
        "    return I\n",
        "\n",
        "  if sign(f(c)) == 0: # Just in case\n",
        "    return [c,c]"
      ],
      "metadata": {
        "id": "1CR6QzRC67Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now create a sequence of approximations to our root, by iterating with a `for` loop."
      ],
      "metadata": {
        "id": "bnU7_edl9rUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_approx = []\n",
        "I = [0.5,1.0]\n",
        "\n",
        "for i in range(10):\n",
        "  x_approx.append(midpoint(I[0],I[1]))\n",
        "  I=bisect(I)\n",
        "\n",
        "print(x_approx)"
      ],
      "metadata": {
        "id": "6HpLq4tC-FAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's keep track of the upper endpoints of our intervals, the lower endpoints of our intervals, and the maximum possible error that we could have had too."
      ],
      "metadata": {
        "id": "BY7t36Zz-o9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_approx = []\n",
        "upper=[]\n",
        "lower=[]\n",
        "error=[]\n",
        "\n",
        "I = [0.5,1.0]\n",
        "\n",
        "for i in range(10):\n",
        "  upper.append(I[1])\n",
        "  lower.append(I[0])\n",
        "  error.append(half_length(I[0],I[1]))\n",
        "  x_approx.append(midpoint(I[0],I[1]))\n",
        "  I=bisect(I)\n",
        "\n",
        "print(\"Upper endpoints:\", upper)\n",
        "print(\"Midpoints:\", x_approx)\n",
        "print(\"Lower endpoints:\", lower)\n",
        "\n",
        "print(\"Errors:\", error)"
      ],
      "metadata": {
        "id": "xqOQONQ7-tth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know how to plot this data, after the last Jupyter Notebook assignment."
      ],
      "metadata": {
        "id": "BHTup_Ek_sn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=[]\n",
        "for i in range(10):\n",
        "  n.append(i)\n",
        "\n",
        "plt.title(\"Upper endpoint, midpoint, and lower endpoints\")\n",
        "plt.plot(n,upper)\n",
        "plt.scatter(n,upper)\n",
        "plt.plot(n,x_approx, color='r')\n",
        "plt.scatter(n,x_approx, color='r')\n",
        "plt.scatter(n,lower, color='g')\n",
        "plt.plot(n,lower, color='g')"
      ],
      "metadata": {
        "id": "ivRz7Y1ZBge1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error is half of the distance between the upper and lower endpoints. If we plot the list of errors, we should see a converging sequence."
      ],
      "metadata": {
        "id": "wPpFO_HsCobe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Sequence of errors\")\n",
        "plt.grid(True)\n",
        "plt.axhline(0, color='black', linewidth=0.8)\n",
        "plt.scatter(n,error, color='r')"
      ],
      "metadata": {
        "id": "-_RlX1kZCnqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To confirm one more time, we can explicitly see what happens when we evaluate our function $f(x)$ to the list of approximations $x_{approx}$."
      ],
      "metadata": {
        "id": "zUga12vGDS6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(f(x_approx[i]))"
      ],
      "metadata": {
        "id": "lj9H0ujbDdRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize, here is the general algorithm for approximating a solution $x=c$ to $f(x)=0$ for a continuous function $f(x)$ using this method.\n",
        "\n",
        "**The Bisection Method**\n",
        "\n",
        "1. Find an interval $I_1=[a,b]$ with $a<b$ where $f(x)$ has a solution $x=c$.\n",
        "2. Calculate the midpoint of $[a,b]$. This will be your first approximation $x_1$ to the solution $x=c$.\n",
        "3. Check which of the intervals $[a,x_1]$ or $[x_1,b]$ contain $x=c$. This will be your new interval $I_2$.\n",
        "4. Repeat, from step 2, with the interval $I_2$ to get an approximation $x_2$ and an interval $I_3$. Continue as many times as needed."
      ],
      "metadata": {
        "id": "Xq99CsWQDsDy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Exercises"
      ],
      "metadata": {
        "id": "NZyhVdPsEwei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1**\n",
        "\n",
        "Use the bisection method to compute $\\sqrt{2}$ with an error of $0.0001$. You should use the function $f(x)=x^2-2$ to do this, with starting interval $I_1=[1,2]$.\n",
        "\n",
        "Answer these questions:\n",
        "\n",
        "a) What is your final approximation and how many iterations did it take to get your approximation?\n",
        "\n",
        "b) Let $\\{\\epsilon_i\\}_{i=1}^\\infty$ be the sequence of maximal possible errors for each approximation. What kind of sequence is this? Does this sequence converge?"
      ],
      "metadata": {
        "id": "q-jLCbzWFCE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2**\n",
        "\n",
        "Make two plots that include the following information:\n",
        "\n",
        "a) One plot should include the upper endpoint, lower endpoint, and midpoints for each interval that you create using the bisection method in Exercise 1.\n",
        "\n",
        "b) The second plot should graph the squares your approximations. This should be a sequence with limit at $y=2$."
      ],
      "metadata": {
        "id": "BZNm0S_cGLhm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 3**\n",
        "\n",
        "Use the bisection method to compute $\\sqrt{5}$ with an error at most $0.0001$."
      ],
      "metadata": {
        "id": "XcQwMZBAyVNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Derivatives and the Newton-Raphson Method"
      ],
      "metadata": {
        "id": "iQ7s1sqpe__9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to continue to look at the solution to the polynomial $f(x)=x^3+x-1$. This time, instead of using the Bisection Method, we'll use a method called *the Newton-Raphson Method*.\n",
        "\n",
        "Our goal is still to find an approximation of the value $x=c$ in the interval $[0.5,1]$ that satisfies $f(c)=0$. The main new idea of this method is to use a \"linear approximation\" to our function at a point near $x=c$.\n",
        "\n",
        "**The Newton-Raphson Method**\n",
        "1. We'll pick a random point $x_1=a_1$ with $0.5\\leq a_1\\leq 1$. This will give us an approximation to the zero of $f(x)$.\n",
        "2. We'll write down the tangent line to $y=f(x)$ at $x_1$. This will give us an approximation to $f(x)$.\n",
        "3. Then we find where the tangent line meets the $x$-axis. This will give us an approximation to $x=c$. We'll call this point where the tangent line meets the $x$-axis $x_2$.\n",
        "4. We'll repeat starting with step 2, until we are close enough."
      ],
      "metadata": {
        "id": "G4uBx9KUID44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's walk through just one step in the process first. We'll start with $x_1=1$, and try to find $x_2$."
      ],
      "metadata": {
        "id": "6v8fGdYFJ49i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define our function again so that it's nearby\n",
        "def f(x):\n",
        "  return x**3+x-1\n",
        "\n",
        "# We'll need the derivative of f(x)\n",
        "def df(x):\n",
        "  return 3*x**2+1\n",
        "\n",
        "# The tangent line at a is y-f(a)=f'(a)(x-a).\n",
        "# We want to know what x value gives y=0.\n",
        "# Solve for this value and compare with the function below.\n",
        "def tangent_line_intercept(a):\n",
        "  return -f(a)/df(a) + a\n",
        "\n",
        "print(tangent_line_intercept(1))"
      ],
      "metadata": {
        "id": "ILCjctBoJ-yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, $x_2=0.75$. It might be nice to see this in a picture."
      ],
      "metadata": {
        "id": "KqlNv7o2m-qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tangent_line(a,x):\n",
        "  return df(a)*(x-a)+f(a)\n",
        "\n",
        "x_inputs=[]\n",
        "y_outputs=[]\n",
        "tl_outputs=[]\n",
        "\n",
        "step=0.01\n",
        "for i in range(80):\n",
        "  x_inputs.append(0.5+i*step)\n",
        "  y_outputs.append(f(x_inputs[i]))\n",
        "  tl_outputs.append(tangent_line(1,x_inputs[i]))\n",
        "\n",
        "plt.grid(True)\n",
        "plt.axhline(0, color='black', linewidth=0.8)\n",
        "\n",
        "plt.plot(x_inputs,tl_outputs)\n",
        "plt.plot(x_inputs,y_outputs)"
      ],
      "metadata": {
        "id": "T6zbsNkMnVHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our approximation going from $x_1=1$ to $x_2=0.75$ certainly seems to have brought us closer to the zero $x=c$. Let's automate this process and see what happens after a few iterations."
      ],
      "metadata": {
        "id": "qCY2a7E0oe-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_approx=[1]\n",
        "\n",
        "for i in range(10):\n",
        "  x_approx.append(tangent_line_intercept(x_approx[i]))\n",
        "\n",
        "print(x_approx)\n"
      ],
      "metadata": {
        "id": "tKj69_zaoq_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the sequence of approximations stabilizes (stops changing) after only a few iterations. Compare this to the example above using the Bisection Method. How should we check if our sequence has converged to something useful? Let's check the value of $f(x)$ at the last approximation."
      ],
      "metadata": {
        "id": "0lyUK3Japioz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f(x_approx[10])"
      ],
      "metadata": {
        "id": "bxWUbukDqX7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Probably you'll see a number which isn't quite $0$ (on my screen I see, 0.000000000000000111022...). But, this is probably the finest approximation to $x=c$ that we'll be able to get with the tools that we're currently using (i.e. floating point decimal values that are native to computer hardware).\n",
        "\n",
        "Let's take the last value of our sequence $x_{approx}[10]$, one of the stabilized terms, as the true value for $x=c$ and plot the error terms for this method. We can calculate the \"exact\" error in this case."
      ],
      "metadata": {
        "id": "2hRSLouiqnBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=[]\n",
        "error=[]\n",
        "for i in range(11):\n",
        "  n.append(i)\n",
        "  error.append(x_approx[i]-x_approx[10])\n",
        "\n",
        "plt.grid(True)\n",
        "plt.title(\"Sequence of errors\")\n",
        "plt.scatter(n,error, color='r')"
      ],
      "metadata": {
        "id": "Hls13_MXpIDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So which of the two methods is better? Weeeeeeeell, it depends.\n",
        "\n",
        "When it works, the Newton-Raphson method will often converge much faster than the Bisection method. Unfortunately, a bad initial guess for the Newton-Raphson method will cause problems (e.g. choosing too coarse of a first guess may lead to an approximation of the wrong zero, or to a sequence that doesn't converge. See the next example of approximating solutions to $g(x)=x^4-3x^2-2=0$)."
      ],
      "metadata": {
        "id": "tWju43OjsGrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def g(x):\n",
        "  return x**4-3*x**2-2\n",
        "\n",
        "def dg(x):\n",
        "  return 4*x**3-6*x\n",
        "\n",
        "def tangent_line_g(a,x):\n",
        "  return dg(a)*(x-a)+g(a)\n",
        "\n",
        "def tangent_line_g_intercept(a):\n",
        "  return -g(a)/dg(a)+a\n",
        "\n",
        "x_inputs=[]\n",
        "y_outputs=[]\n",
        "tangent_1=[]\n",
        "tangent_2=[]\n",
        "tangent_3=[]\n",
        "\n",
        "t1=1\n",
        "t2=tangent_line_g_intercept(t1)\n",
        "t3=tangent_line_g_intercept(t2)\n",
        "print(\"t2 is:\", t2)\n",
        "print(\"t3 is:\", t3)\n",
        "\n",
        "step=0.01\n",
        "for i in range(400):\n",
        "  x_inputs.append(-2+i*step)\n",
        "  y_outputs.append(g(x_inputs[i]))\n",
        "  tangent_1.append(tangent_line_g(t1,x_inputs[i]))\n",
        "  tangent_2.append(tangent_line_g(t2,x_inputs[i]))\n",
        "\n",
        "plt.grid(True)\n",
        "plt.ylim(-5,2)\n",
        "plt.axhline(0,color='black', linewidth=0.8)\n",
        "plt.plot(x_inputs,y_outputs, color='black')\n",
        "plt.plot(x_inputs,tangent_1, color='r')\n",
        "plt.plot(x_inputs,tangent_2, color='blue')"
      ],
      "metadata": {
        "id": "_9RbI_i5s7F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the sequence we'd generate using the Newton-Raphson method with initial guess $x_1=1$ is $x_2=-1$, $x_3=1$, $x_4=-1$,..., $x_n=(-1)^{n+1}$. We know this sequence doesn't converge, since it alternates infinitely often between 1 and -1."
      ],
      "metadata": {
        "id": "b5dsKve2vOub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Exercises"
      ],
      "metadata": {
        "id": "oV1YYvZ_wD9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 4**\n",
        "\n",
        "Use the Newton-Raphson method to compute $\\sqrt{2}$ with an error of $0.0001$. You should use the function $f(x)=x^2-2$ to do this, with starting approximation $x_1=2$.\n",
        "\n",
        "Answer these questions:\n",
        "\n",
        "a) What is your final approximation and how many iterations did it take to get your approximation?\n",
        "\n",
        "b) Did it take less iterations or more iterations to reach your final approximation compared to using the Bisection method?"
      ],
      "metadata": {
        "id": "zJoobYBfwKw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 5**\n",
        "\n",
        "Use the Newton-Raphson method to compute an approximation to $\\sqrt[3]{13}$. Make sure your approximation has an error at most $0.0001$ (this means the 4th decimal place should stabilize in your approximations)."
      ],
      "metadata": {
        "id": "YJizS30pxwCI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 6**\n",
        "\n",
        "Use the Newton-Raphson method to find an approximate solution to the equation $e^x-x=2$. Your solution should be accurate with an error at most $0.0001$."
      ],
      "metadata": {
        "id": "PasArT2fwnjL"
      }
    }
  ]
}